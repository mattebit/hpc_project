#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <unistd.h>
#include <string.h>
#include <stdbool.h>
#include <omp.h>
#include <stdint.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <stddef.h>
#include <limits.h>

#define MAX_EDGE_VALUE UINT16_MAX
#define ALIGNMENT 64 

// Global variables
int NODE_COUNT = 20;
char* GRAPH_PATH;
int MY_NODES_FROM = 0;
int MY_NODES_TO = 0;

typedef struct {
    uint16_t weight;
    int from;
    int to;
} Edge;

typedef struct {
    int process_count;
    int world_rank;
    MPI_Datatype edge_type;
    MPI_Op min_edge_op;
} MPIContext;

// ---------------------------------------------------------------------------
// Helper: Aligned Memory Allocation
// ---------------------------------------------------------------------------
void* aligned_malloc(size_t size) {
    void* ptr;
    if (posix_memalign(&ptr, ALIGNMENT, size) != 0) {
        fprintf(stderr, "Aligned allocation failed\n");
        exit(EXIT_FAILURE);
    }
    return ptr;
}

// ---------------------------------------------------------------------------
// Union Find (Optimized with Explicit Flattening)
// ---------------------------------------------------------------------------
void init_union_find(int* parent, int* rank, int n) {
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < n; i++) {
        parent[i] = i;
        rank[i] = 0;
    }
}

// Standard find (used in serial parts)
int find(int* parent, int node) {
    int root = node;
    while (root != parent[root]) root = parent[root];
    while (node != root) {
        int next = parent[node];
        parent[node] = root;
        node = next;
    }
    return root;
}

// Flatten trees in parallel so subsequent lookups are O(1)
void flatten_trees(int* parent, int n) {
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < n; i++) {
        int root = i;
        while (root != parent[root]) root = parent[root];
        
        // Path compression
        int node = i;
        while (node != root) {
            int next = parent[node];
            parent[node] = root;
            node = next;
        }
    }
}

void union_sets(int* parent, int* rank, int node1, int node2) {
    int root1 = find(parent, node1);
    int root2 = find(parent, node2);
    if (root1 != root2) {
        if (rank[root1] > rank[root2]) {
            parent[root2] = root1;
        } else if (rank[root1] < rank[root2]) {
            parent[root1] = root2;
        } else {
            parent[root2] = root1;
            rank[root1]++;
        }
    }
}

// ---------------------------------------------------------------------------
// Matrix Operations (Flattened)
// ---------------------------------------------------------------------------
static inline void set_edge(uint16_t* matrix, int row, int col, uint16_t val) {
    matrix[row * NODE_COUNT + col] = val;
}

// ---------------------------------------------------------------------------
// High Performance I/O
// ---------------------------------------------------------------------------
static inline int fast_atoi(const char **str) {
    int val = 0;
    while (**str < '0' || **str > '9') (*str)++; 
    while (**str >= '0' && **str <= '9') {
        val = (val * 10) + (**str - '0');
        (*str)++;
    }
    return val;
}

uint16_t* readGraphFast(const char* filename) {
    int fd = open(filename, O_RDONLY);
    if (fd == -1) { perror("Error opening file"); exit(1); }

    struct stat sb;
    if (fstat(fd, &sb) == -1) { perror("Error getting file size"); exit(1); }

    char* file_data = mmap(NULL, sb.st_size, PROT_READ, MAP_PRIVATE, fd, 0);
    if (file_data == MAP_FAILED) { perror("mmap failed"); exit(1); }

    const char* ptr = file_data;
    int V = fast_atoi(&ptr);
    int E = fast_atoi(&ptr);
    NODE_COUNT = V;

    size_t size = (size_t)NODE_COUNT * NODE_COUNT * sizeof(uint16_t);
    uint16_t* graph = (uint16_t*)aligned_malloc(size);
    
    // Init to MAX
    #pragma omp parallel for schedule(static)
    for(size_t i=0; i < (size_t)NODE_COUNT * NODE_COUNT; i++) {
        graph[i] = MAX_EDGE_VALUE;
    }

    for (int i = 0; i < E; i++) {
        int src = fast_atoi(&ptr);
        int dest = fast_atoi(&ptr);
        uint16_t weight = (uint16_t)fast_atoi(&ptr);

        if (graph[src * NODE_COUNT + dest] > weight) {
            graph[src * NODE_COUNT + dest] = weight;
            graph[dest * NODE_COUNT + src] = weight;
        }
    }

    munmap(file_data, sb.st_size);
    close(fd);
    return graph;
}

// ---------------------------------------------------------------------------
// Core Logic
// ---------------------------------------------------------------------------

void find_local_minimum_edges(int vertex_per_process, uint16_t* graph, int* lightest_edges) {
    // FIX 1: Schedule static avoids atomic overhead
    #pragma omp parallel for schedule(static)
    for (int i = MY_NODES_FROM; i < MY_NODES_TO; i++) {
        uint16_t local_min_weight = MAX_EDGE_VALUE;
        int local_min_id = -1;
        const uint16_t* row_ptr = &graph[i * NODE_COUNT];

        // FIX 2: Pure SIMD scan first (branchless)
        #pragma omp simd reduction(min:local_min_weight)
        for (int j = 0; j < NODE_COUNT; j++) {
            if (i == j) continue;
            uint16_t w = row_ptr[j];
            if (w < local_min_weight) local_min_weight = w;
        }

        if (local_min_weight != MAX_EDGE_VALUE) {
            for (int j = 0; j < NODE_COUNT; j++) {
                if (i != j && row_ptr[j] == local_min_weight) {
                    local_min_id = j;
                    break;
                }
            }
        }
        lightest_edges[i - MY_NODES_FROM] = local_min_id;
    }
}

void find_min_components(int* parent, uint16_t* graph, Edge* min_edges) {
    // Reset edges
    for(int k=0; k < (MY_NODES_TO - MY_NODES_FROM); k++) {
        min_edges[k] = (Edge){MAX_EDGE_VALUE, -1, -1};
    }

    #pragma omp parallel for schedule(static)
    for (int i = MY_NODES_FROM; i < MY_NODES_TO; i++) {
        int root_i = parent[i]; 
        
        // Get pointer to the row. 
        // We cast to non-const because we WILL modify it to prune edges.
        uint16_t* row_ptr = &graph[i * NODE_COUNT];

        uint16_t best_w = MAX_EDGE_VALUE;
        int best_to = -1;

        for (int j = 0; j < NODE_COUNT; j++) {
            uint16_t w = row_ptr[j];
            
            // 1. Standard Skip
            if (w == MAX_EDGE_VALUE) continue;

            // 2. Optimization: Only check component if weight beats our current best
            if (w < best_w) {
                
                // 3. The Expensive Check (Random Memory Access)
                // We use 'parent' directly because we flattened trees earlier
                if (root_i == parent[j]) {
                    
                    // --- PRUNING HAPPENS HERE ---
                    // This edge connects two nodes in the same component.
                    // It is useless forever. Mark it as MAX so we never 
                    // check 'parent[j]' for this edge again.
                    row_ptr[j] = MAX_EDGE_VALUE; 
                    
                } else {
                    // Valid edge to a different component
                    best_w = w;
                    best_to = j;
                }
            }
        }

        if (best_to != -1) {
            min_edges[i - MY_NODES_FROM] = (Edge){best_w, i, best_to};
        }
    }
}

void min_edge_reduce(void* in, void* inout, int* len, MPI_Datatype* datatype) {
    Edge* in_edges = (Edge*)in;
    Edge* inout_edges = (Edge*)inout;
    for (int i = 0; i < *len; i++) {
        if (in_edges[i].weight < inout_edges[i].weight) {
            inout_edges[i] = in_edges[i];
        }
    }
}

// ---------------------------------------------------------------------------
// Main
// ---------------------------------------------------------------------------

MPIContext init_mpi() {
    MPIContext ctx;
    MPI_Init(NULL, NULL);
    MPI_Comm_size(MPI_COMM_WORLD, &ctx.process_count);
    MPI_Comm_rank(MPI_COMM_WORLD, &ctx.world_rank);

    MPI_Datatype types[3] = {MPI_UINT16_T, MPI_INT, MPI_INT};
    int blocklens[3] = {1, 1, 1};
    MPI_Aint disps[3];
    disps[0] = offsetof(Edge, weight);
    disps[1] = offsetof(Edge, from);
    disps[2] = offsetof(Edge, to);

    MPI_Type_create_struct(3, blocklens, disps, types, &ctx.edge_type);
    MPI_Type_commit(&ctx.edge_type);
    MPI_Op_create((MPI_User_function*)min_edge_reduce, 1, &ctx.min_edge_op);
    return ctx;
}

int main(int argc, char** argv) {
    MPIContext mpi = init_mpi();
    
    // I/O & Setup Phase
    if (mpi.world_rank == 0) {
        if (argc < 3) exit(1);
        NODE_COUNT = atoi(argv[1]);
        GRAPH_PATH = argv[2];
    }
    MPI_Bcast(&NODE_COUNT, 1, MPI_INT, 0, MPI_COMM_WORLD);

    size_t size = (size_t)NODE_COUNT * NODE_COUNT * sizeof(uint16_t);
    uint16_t* graph = (uint16_t*)aligned_malloc(size);

    if (mpi.world_rank == 0) {
        free(graph);
        graph = readGraphFast(GRAPH_PATH);
    }
    
    // Fast Bulk Broadcast
    if (size < INT_MAX) {
        MPI_Bcast(graph, NODE_COUNT * NODE_COUNT, MPI_UINT16_T, 0, MPI_COMM_WORLD);
    } else {
        // Chunked fallback for huge graphs
        size_t sent = 0;
        int chunk = 1024 * 1024 * 512; 
        while(sent < size/2) { // size/2 because uint16 is 2 bytes
             int c = (size/2 - sent > chunk) ? chunk : (size/2 - sent);
             MPI_Bcast(graph + sent, c, MPI_UINT16_T, 0, MPI_COMM_WORLD);
             sent += c;
        }
    }

    // Setup Distribution
    int v_per_proc = NODE_COUNT / mpi.process_count;
    int rem = NODE_COUNT % mpi.process_count;
    MY_NODES_FROM = mpi.world_rank * v_per_proc + (mpi.world_rank < rem ? mpi.world_rank : rem);
    if (mpi.world_rank < rem) v_per_proc++;
    MY_NODES_TO = MY_NODES_FROM + v_per_proc;

    int* parent = malloc(NODE_COUNT * sizeof(int));
    int* rank = calloc(NODE_COUNT, sizeof(int));
    uint16_t* min_graph = calloc(NODE_COUNT * NODE_COUNT, sizeof(uint16_t));
    init_union_find(parent, rank, NODE_COUNT);
    
    // Wait for all nodes to be ready before starting timer
    MPI_Barrier(MPI_COMM_WORLD);
    double start_compute = MPI_Wtime();

    // ---------------------------------------------------------
    // Algorithm Start
    // ---------------------------------------------------------
    
    // Iteration 1: Nearest Neighbor (Every node is a component)
    int* local_mins = malloc(v_per_proc * sizeof(int));
    int* global_mins = malloc(NODE_COUNT * sizeof(int));
    
    find_local_minimum_edges(v_per_proc, graph, local_mins);

    // Manual Allgatherv via Reduce (Simple & Fast for 1D array)
    int* temp_buf = malloc(NODE_COUNT * sizeof(int));
    for(int i=0; i<NODE_COUNT; i++) temp_buf[i] = -1;
    memcpy(&temp_buf[MY_NODES_FROM], local_mins, v_per_proc * sizeof(int));
    
    for(int i=0; i<NODE_COUNT; i++) global_mins[i] = -1; // reset recv buf
    MPI_Allreduce(temp_buf, global_mins, NODE_COUNT, MPI_INT, MPI_MAX, MPI_COMM_WORLD);
    free(temp_buf);

    #pragma omp parallel for schedule(static)
    for (int i = 0; i < NODE_COUNT; i++) {
        if (global_mins[i] != -1) {
            set_edge(min_graph, i, global_mins[i], 1);
            set_edge(min_graph, global_mins[i], i, 1);
        }
    }
    // Serial union is fast enough for Iter 1
    for(int i=0; i<NODE_COUNT; i++) if(global_mins[i] != -1) union_sets(parent, rank, i, global_mins[i]);

    // Subsequent Iterations
    Edge* local_comp_edges = malloc(NODE_COUNT * sizeof(Edge));
    Edge* global_comp_edges = malloc(NODE_COUNT * sizeof(Edge));
    Edge* my_node_edges = malloc(v_per_proc * sizeof(Edge));

    while (true) {
        // Flatten trees to make lookups O(1) in the parallel loop
        flatten_trees(parent, NODE_COUNT);

        int comps = 0;
        // Count components (could be optimized, but fast enough)
        for(int i=0; i<NODE_COUNT; i++) if(parent[i] == i) comps++;
        if(comps <= 1) break;

        // Reset buffers
        for(int i=0; i<NODE_COUNT; i++) {
            local_comp_edges[i] = (Edge){MAX_EDGE_VALUE, -1, -1};
            global_comp_edges[i] = (Edge){MAX_EDGE_VALUE, -1, -1};
        }

        find_min_components(parent, graph, my_node_edges);

        // Local Reduction
        for(int i=0; i < v_per_proc; i++) {
            if (my_node_edges[i].weight != MAX_EDGE_VALUE) {
                // parent[x] is guaranteed to be root due to flatten_trees
                int root = parent[MY_NODES_FROM + i];
                if (my_node_edges[i].weight < local_comp_edges[root].weight) {
                    local_comp_edges[root] = my_node_edges[i];
                }
            }
        }

        MPI_Allreduce(local_comp_edges, global_comp_edges, NODE_COUNT, mpi.edge_type, mpi.min_edge_op, MPI_COMM_WORLD);

        bool any_merge = false;
        for(int i=0; i<NODE_COUNT; i++) {
            Edge e = global_comp_edges[i];
            if (e.weight != MAX_EDGE_VALUE) {
                int root1 = find(parent, e.from);
                int root2 = find(parent, e.to);
                if (root1 != root2) {
                    union_sets(parent, rank, root1, root2);
                    set_edge(min_graph, e.from, e.to, 1);
                    set_edge(min_graph, e.to, e.from, 1);
                    any_merge = true;
                }
            }
        }
        if (!any_merge) break;
    }

    // ---------------------------------------------------------
    // Report
    // ---------------------------------------------------------
    if (mpi.world_rank == 0) {
        double end_compute = MPI_Wtime();
        long weight = 0;
        for(int i=0; i<NODE_COUNT; i++) {
            for(int j=0; j<i; j++) {
                if(min_graph[i*NODE_COUNT + j]) weight += graph[i*NODE_COUNT + j];
            }
        }
        printf("Computation Time: %f\n", end_compute - start_compute);
        printf("Total MST Weight: %ld\n", weight);
    }

    // Cleanup
    free(graph); free(min_graph); free(parent); free(rank);
    free(local_mins); free(global_mins);
    free(local_comp_edges); free(global_comp_edges); free(my_node_edges);
    
    MPI_Type_free(&mpi.edge_type);
    MPI_Op_free(&mpi.min_edge_op);
    MPI_Finalize();
    return 0;
}